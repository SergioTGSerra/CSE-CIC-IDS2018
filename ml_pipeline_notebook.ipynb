{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline for Network Intrusion Detection\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for network intrusion detection using the CSE-CIC-IDS2018 dataset. The pipeline includes data preprocessing, exploratory data analysis, feature engineering, and various machine learning models for classification.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Preprocessing and Transformation](#preprocessing)\n",
    "3. [Exploratory Data Analysis (EDA)](#eda)\n",
    "4. [Feature Engineering](#feature_engineering)\n",
    "5. [Machine Learning Phase](#ml_phase)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Model Usage](#usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup and Imports\n",
    "\n",
    "First, we'll import all the necessary libraries for our machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve, auc, \n",
    "    silhouette_score, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Advanced ML models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## 2. Data Preprocessing and Transformation\n",
    "\n",
    "In this section, we'll load the dataset, inspect it, clean it, and prepare it for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path='./data/'):\n",
    "    \"\"\"\n",
    "    Load CSV files from the specified directory and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame from all CSV files\n",
    "    \"\"\"\n",
    "    # List all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(csv_files)} CSV files in {data_path}\")\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Load each CSV file\n",
    "    for file in tqdm(csv_files, desc=\"Loading files\"):\n",
    "        try:\n",
    "            file_path = os.path.join(data_path, file)\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            print(f\"Loaded {file}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Combined dataset: {combined_df.shape[0]} rows, {combined_df.shape[1]} columns\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        raise ValueError(\"No valid data files found. Check dataset contents.\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = load_dataset()\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"Using sample data for demonstration purposes...\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create sample features\n",
    "    sample_data = {\n",
    "        'Flow Duration': np.random.randint(1, 100000, n_samples),\n",
    "        'Total Fwd Packets': np.random.randint(1, 100, n_samples),\n",
    "        'Total Backward Packets': np.random.randint(1, 100, n_samples),\n",
    "        'Total Length of Fwd Packets': np.random.randint(1, 10000, n_samples),\n",
    "        'Total Length of Bwd Packets': np.random.randint(1, 10000, n_samples),\n",
    "        'Fwd Packet Length Max': np.random.randint(1, 1500, n_samples),\n",
    "        'Fwd Packet Length Min': np.random.randint(0, 100, n_samples),\n",
    "        'Fwd Packet Length Mean': np.random.uniform(10, 500, n_samples),\n",
    "        'Bwd Packet Length Max': np.random.randint(1, 1500, n_samples),\n",
    "        'Bwd Packet Length Min': np.random.randint(0, 100, n_samples),\n",
    "        'Bwd Packet Length Mean': np.random.uniform(10, 500, n_samples),\n",
    "        'Flow Bytes/s': np.random.uniform(0, 10000, n_samples),\n",
    "        'Flow Packets/s': np.random.uniform(0, 1000, n_samples),\n",
    "        'Flow IAT Mean': np.random.uniform(0, 1000, n_samples),\n",
    "        'Flow IAT Std': np.random.uniform(0, 500, n_samples),\n",
    "        'Flow IAT Max': np.random.uniform(0, 2000, n_samples),\n",
    "        'Flow IAT Min': np.random.uniform(0, 100, n_samples),\n",
    "        'Fwd Header Length.1': np.random.randint(20, 100, n_samples),  # Duplicate column\n",
    "        'Fwd Header Length': np.random.randint(20, 100, n_samples),\n",
    "        'Protocol': np.random.choice(['TCP', 'UDP', 'ICMP'], n_samples),\n",
    "        'Destination Port': np.random.choice([80, 443, 22, 53, 8080], n_samples),\n",
    "        'Label': np.random.choice(['BENIGN', 'DoS', 'PortScan', 'Brute Force', 'Web Attack'], n_samples, \n",
    "                                 p=[0.7, 0.1, 0.1, 0.05, 0.05])\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample dataset: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Inspect Data Types and Count Non-Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "df.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()\n",
    "\n",
    "# Count non-null values for each column\n",
    "print(\"\\nNon-null value counts:\")\n",
    "non_null_counts = df.count()\n",
    "print(non_null_counts)\n",
    "\n",
    "# Calculate percentage of non-null values\n",
    "non_null_percentage = (non_null_counts / len(df)) * 100\n",
    "print(\"\\nPercentage of non-null values:\")\n",
    "print(non_null_percentage)\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize column names by removing leading/trailing whitespace,\n",
    "    replacing spaces with underscores, and converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized column names\n",
    "    \"\"\"\n",
    "    # Store original column names for reference\n",
    "    original_columns = df.columns.tolist()\n",
    "    \n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    \n",
    "    # Print mapping of original to normalized column names\n",
    "    print(\"Column name mapping:\")\n",
    "    for orig, norm in zip(original_columns, df.columns):\n",
    "        if orig != norm:\n",
    "            print(f\"  {orig} -> {norm}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Normalize column names\n",
    "df = normalize_column_names(df)\n",
    "\n",
    "# Display the first few rows with normalized column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Identify and Remove Duplicate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify and remove duplicate columns\n",
    "def remove_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Identify and remove duplicate columns from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicate columns removed\n",
    "    \"\"\"\n",
    "    # Get list of all columns\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Initialize list to store duplicate columns\n",
    "    duplicate_columns = []\n",
    "    \n",
    "    # Check for columns ending with '.1', '.2', etc.\n",
    "    for col in columns:\n",
    "        if col.endswith(('.1', '.2', '.3', '.4', '.5')):\n",
    "            base_col = col.rsplit('.', 1)[0]\n",
    "            if base_col in columns:\n",
    "                # Check if the columns are actually duplicates\n",
    "                if df[col].equals(df[base_col]):\n",
    "                    duplicate_columns.append(col)\n",
    "                    print(f\"Found duplicate column: {col} (duplicate of {base_col})\")\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    if duplicate_columns:\n",
    "        df = df.drop(columns=duplicate_columns)\n",
    "        print(f\"Removed {len(duplicate_columns)} duplicate columns\")\n",
    "    else:\n",
    "        print(\"No duplicate columns found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Remove duplicate columns\n",
    "df = remove_duplicate_columns(df)\n",
    "\n",
    "# Display the shape of the DataFrame after removing duplicate columns\n",
    "print(f\"DataFrame shape after removing duplicate columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Convert Categorical Features using Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical features\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Encode categorical features using Label Encoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded categorical features\n",
    "        dict: Dictionary mapping column names to their respective label encoders\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Initialize dictionary to store label encoders\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Identify categorical columns (excluding the target variable)\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if 'label' in categorical_columns:\n",
    "        categorical_columns.remove('label')\n",
    "    \n",
    "    print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
    "    \n",
    "    # Encode each categorical column\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    # Handle the target variable separately\n",
    "    if 'label' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        label_encoders['label'] = le\n",
    "        print(f\"Encoded label: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded, label_encoders = encode_categorical_features(df)\n",
    "\n",
    "# Display the first few rows of the encoded DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Convert All Data Types to Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all data types to float\n",
    "def convert_to_float(df):\n",
    "    \"\"\"\n",
    "    Convert all columns (except the target variable) to float.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all columns converted to float\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_float = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "    \n",
    "    # Convert each column to float\n",
    "    for col in columns:\n",
    "        df_float[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
    "    \n",
    "    return df_float\n",
    "\n",
    "# Convert all data types to float\n",
    "df_float = convert_to_float(df_encoded)\n",
    "\n",
    "# Display the data types after conversion\n",
    "print(\"Data types after conversion:\")\n",
    "print(df_float.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Replace Infinite Values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace infinite values with NaN\n",
    "def replace_inf_with_nan(df):\n",
    "    \"\"\"\n",
    "    Replace infinite values with NaN in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with infinite values replaced with NaN\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_inf = df.copy()\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    df_no_inf = df_no_inf.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Count NaN values after replacement\n",
    "    nan_counts = df_no_inf.isna().sum()\n",
    "    print(\"NaN counts after replacing infinite values:\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "    \n",
    "    return df_no_inf\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_no_inf = replace_inf_with_nan(df_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Handle Missing Values (NaN) by Imputing with the Column Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values with column mean\n",
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values (NaN) with the column mean.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values imputed\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "    \n",
    "    # Impute missing values with column mean\n",
    "    for col in columns:\n",
    "        if df_imputed[col].isna().any():\n",
    "            mean_value = df_imputed[col].mean()\n",
    "            df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
    "            print(f\"Imputed {df_imputed[col].isna().sum()} missing values in {col} with mean: {mean_value:.4f}\")\n",
    "    \n",
    "    # Verify that there are no more missing values\n",
    "    nan_counts = df_imputed.isna().sum()\n",
    "    if nan_counts.sum() > 0:\n",
    "        print(\"Warning: There are still missing values in the DataFrame\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    else:\n",
    "        print(\"All missing values have been imputed\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Impute missing values with column mean\n",
    "df_imputed = impute_missing_values(df_no_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Replace Nonsensical Negative Values with the Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace nonsensical negative values with the median\n",
    "def replace_negative_values(df):\n",
    "    \"\"\"\n",
    "    Replace nonsensical negative values in specific columns with the median.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with nonsensical negative values replaced\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_neg = df.copy()\n",
    "    \n",
    "    # List of columns that should not have negative values\n",
    "    # These are typically columns representing counts, durations, lengths, etc.\n",
    "    non_negative_columns = [\n",
    "        col for col in df.columns if any(keyword in col.lower() for keyword in \n",
    "                                        ['duration', 'length', 'packets', 'bytes', 'count', 'min', 'max', 'mean'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(non_negative_columns)} columns that should not have negative values\")\n",
    "    \n",
    "    # Replace negative values with the median in each column\n",
    "    for col in non_negative_columns:\n",
    "        if col in df_no_neg.columns and (df_no_neg[col] < 0).any():\n",
    "            neg_count = (df_no_neg[col] < 0).sum()\n",
    "            median_value = df_no_neg[df_no_neg[col] >= 0][col].median()\n",
    "            df_no_neg.loc[df_no_neg[col] < 0, col] = median_value\n",
    "            print(f\"Replaced {neg_count} negative values in {col} with median: {median_value:.4f}\")\n",
    "    \n",
    "    return df_no_neg\n",
    "\n",
    "# Replace nonsensical negative values with the median\n",
    "df_no_neg = replace_negative_values(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Detect and Remove Outliers using Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and remove outliers using Isolation Forest\n",
    "def remove_outliers(df, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect and remove outliers using the Isolation Forest algorithm.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        contamination (float): The proportion of outliers in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_outliers = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Initialize and fit the Isolation Forest model\n",
    "    isolation_forest = IsolationForest(contamination=contamination, random_state=RANDOM_STATE)\n",
    "    outlier_pred = isolation_forest.fit_predict(X)\n",
    "    \n",
    "    # Count outliers\n",
    "    outlier_count = (outlier_pred == -1).sum()\n",
    "    print(f\"Detected {outlier_count} outliers ({outlier_count/len(df)*100:.2f}% of the dataset)\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_no_outliers = df_no_outliers[outlier_pred == 1]\n",
    "    print(f\"DataFrame shape after removing outliers: {df_no_outliers.shape}\")\n",
    "    \n",
    "    return df_no_outliers\n",
    "\n",
    "# Detect and remove outliers\n",
    "df_no_outliers = remove_outliers(df_no_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Scale All Numerical Features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale numerical features\n",
    "def scale_features(df):\n",
    "    \"\"\"\n",
    "    Scale all numerical features using StandardScaler.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with scaled features\n",
    "        StandardScaler: Fitted scaler object\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Initialize and fit the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create a new DataFrame with scaled features\n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    # Add the target variable back to the DataFrame\n",
    "    if 'label' in df.columns:\n",
    "        df_scaled['label'] = df['label'].values\n",
    "    \n",
    "    print(f\"Features scaled using StandardScaler\")\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Scale numerical features\n",
    "df_scaled, scaler = scale_features(df_no_outliers)\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Summary of Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of data preprocessing steps\n",
    "print(\"Summary of Data Preprocessing Steps:\")\n",
    "print(f\"1. Original DataFrame shape: {df.shape}\")\n",
    "print(f\"2. After removing duplicate columns: {df_encoded.shape}\")\n",
    "print(f\"3. After handling missing values: {df_imputed.shape}\")\n",
    "print(f\"4. After removing outliers: {df_no_outliers.shape}\")\n",
    "print(f\"5. Final DataFrame shape: {df_scaled.shape}\")\n",
    "\n",
    "# Display class distribution\n",
    "if 'label' in df_scaled.columns:\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    class_counts = df_scaled['label'].value_counts()\n",
    "    class_percentages = class_counts / len(df_scaled) * 100\n",
    "    \n",
    "    # Create a DataFrame to display class distribution\n",
    "    class_distribution = pd.DataFrame({\n",
    "        'Count': class_counts,\n",
    "        'Percentage': class_percentages\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to original class names\n",
    "    if 'label' in label_encoders:\n",
    "        class_distribution.index = [label_encoders['label'].inverse_transform([i])[0] for i in class_distribution.index]\n",
    "    \n",
    "    print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we'll perform exploratory data analysis to understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute descriptive statistics\n",
    "def compute_descriptive_statistics(df):\n",
    "    \"\"\"\n",
    "    Compute descriptive statistics for the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with descriptive statistics\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Compute descriptive statistics\n",
    "    stats = X.describe(percentiles=[0.25, 0.5, 0.75])\n",
    "    \n",
    "    # Add mode to the statistics\n",
    "    mode = X.mode().iloc[0]\n",
    "    stats.loc['mode'] = mode\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compute descriptive statistics\n",
    "stats = compute_descriptive_statistics(df_no_outliers)\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize descriptive statistics\n",
    "def visualize_descriptive_statistics(stats):\n",
    "    \"\"\"\n",
    "    Visualize descriptive statistics using bar plots.\n",
    "    \n",
    "    Args:\n",
    "        stats (pd.DataFrame): DataFrame with descriptive statistics\n",
    "    \"\"\"\n",
    "    # Select a subset of columns for visualization\n",
    "    selected_columns = stats.columns[:5]  # Select first 5 columns\n",
    "    \n",
    "    # Select statistics to visualize\n",
    "    selected_stats = ['min', '25%', '50%', '75%', 'max', 'mean', 'std']\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(len(selected_columns), 1, figsize=(12, 4 * len(selected_columns)))\n",
    "    \n",
    "    # Plot each column's statistics\n",
    "    for i, col in enumerate(selected_columns):\n",
    "        ax = axes[i] if len(selected_columns) > 1 else axes\n",
    "        \n",
    "        # Extract statistics for the column\n",
    "        col_stats = stats.loc[selected_stats, col]\n",
    "        \n",
    "        # Create bar plot\n",
    "        col_stats.plot(kind='bar', ax=ax)\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Descriptive Statistics for {col}')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_xlabel('Statistic')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, v in enumerate(col_stats):\n",
    "            ax.text(j, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize descriptive statistics\n",
    "visualize_descriptive_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot histograms for selected features\n",
    "def plot_histograms(df, n_cols=5):\n",
    "    \"\"\"\n",
    "    Plot histograms for selected features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        n_cols (int): Number of columns to visualize\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Select a subset of columns for visualization\n",
    "    selected_columns = X.columns[:n_cols]  # Select first n_cols columns\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))\n",
    "    \n",
    "    # Plot histogram for each column\n",
    "    for i, col in enumerate(selected_columns):\n",
    "        ax = axes[i] if n_cols > 1 else axes\n",
    "        \n",
    "        # Plot histogram\n",
    "        sns.histplot(df[col], kde=True, ax=ax)\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Distribution of {col}')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot histograms for selected features\n",
    "plot_histograms(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize Different Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize different distributions\n",
    "def visualize_distributions(df):\n",
    "    \"\"\"\n",
    "    Visualize and analyze different distributions in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1. Normal Distribution\n",
    "    # Find a column that might follow a normal distribution (using skewness close to 0)\n",
    "    skewness = X.skew().abs()\n",
    "    normal_col = skewness.sort_values().index[0]\n",
    "    \n",
    "    sns.histplot(df[normal_col], kde=True, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'Normal Distribution: {normal_col}')\n",
    "    \n",
    "    # Add a perfect normal distribution for comparison\n",
    "    x = np.linspace(df[normal_col].min(), df[normal_col].max(), 100)\n",
    "    mean = df[normal_col].mean()\n",
    "    std = df[normal_col].std()\n",
    "    y = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "    axes[0, 0].plot(x, y * len(df) * (df[normal_col].max() - df[normal_col].min()) / 10, 'r--', label='Perfect Normal')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Binomial Distribution (e.g., for flag columns)\n",
    "    # Find a column that might follow a binomial distribution (using unique values count = 2)\n",
    "    binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
    "    if binary_cols:\n",
    "        binomial_col = binary_cols[0]\n",
    "        sns.countplot(x=df[binomial_col], ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(f'Binomial Distribution: {binomial_col}')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No binary columns found', ha='center', va='center')\n",
    "        axes[0, 1].set_title('Binomial Distribution')\n",
    "    \n",
    "    # 3. Poisson Distribution (e.g., for packet counts)\n",
    "    # Find a column that might follow a Poisson distribution (using columns with 'count' or 'packets' in name)\n",
    "    poisson_cols = [col for col in X.columns if 'count' in col.lower() or 'packets' in col.lower()]\n",
    "    if poisson_cols:\n",
    "        poisson_col = poisson_cols[0]\n",
    "        sns.histplot(df[poisson_col], kde=False, ax=axes[1, 0], discrete=True)\n",
    "        axes[1, 0].set_title(f'Poisson Distribution: {poisson_col}')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No count/packets columns found', ha='center', va='center')\n",
    "        axes[1, 0].set_title('Poisson Distribution')\n",
    "    \n",
    "    # 4. Student's t-Distribution\n",
    "    # Use a column with high kurtosis for t-distribution\n",
    "    kurtosis = X.kurtosis()\n",
    "    t_col = kurtosis.sort_values(ascending=False).index[0]\n",
    "    \n",
    "    sns.histplot(df[t_col], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Student\\'s t-Distribution: {t_col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize different distributions\n",
    "visualize_distributions(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a scatter plot matrix\n",
    "def create_scatter_plot_matrix(df, n_cols=5):\n",
    "    \"\"\"\n",
    "    Create a scatter plot matrix for selected features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        n_cols (int): Number of columns to visualize\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Select a subset of columns for visualization\n",
    "    selected_columns = X.columns[:n_cols]  # Select first n_cols columns\n",
    "    \n",
    "    # Create a scatter plot matrix\n",
    "    sns.pairplot(df[list(selected_columns) + ['label']] if 'label' in df.columns else df[selected_columns], \n",
    "                hue='label' if 'label' in df.columns else None)\n",
    "    plt.suptitle('Scatter Plot Matrix', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Create a scatter plot matrix\n",
    "create_scatter_plot_matrix(df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a correlation matrix heatmap\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create a correlation matrix heatmap.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all columns including the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a correlation matrix heatmap\n",
    "create_correlation_matrix(df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a filtered heatmap for high correlations\n",
    "def create_filtered_heatmap(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Create a filtered heatmap for high correlations.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): Correlation threshold\n",
    "    \"\"\"\n",
    "    # Get list of all columns including the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Create a filtered correlation matrix\n",
    "    filtered_corr = corr_matrix.mask(np.abs(corr_matrix) < threshold)\n",
    "    filtered_corr = filtered_corr.mask(mask)\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5, fmt='.2f')\n",
    "    plt.title(f'Filtered Correlation Matrix Heatmap (|corr| >= {threshold})')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a filtered heatmap for high correlations (threshold = 0.5)\n",
    "create_filtered_heatmap(df_no_outliers, threshold=0.5)\n",
    "\n",
    "# Create a filtered heatmap for very high correlations (threshold = 0.9)\n",
    "create_filtered_heatmap(df_no_outliers, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Outlier Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create boxplots for all features\n",
    "def create_boxplots(df, n_cols=5):\n",
    "    \"\"\"\n",
    "    Create boxplots for all features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        n_cols (int): Number of columns to visualize per row\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Calculate number of rows needed\n",
    "    n_rows = (len(X.columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    \n",
    "    # Flatten axes array for easy indexing\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    # Plot boxplot for each column\n",
    "    for i, col in enumerate(X.columns):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x=df[col], ax=axes[i])\n",
    "            axes[i].set_title(f'Boxplot of {col}')\n",
    "            axes[i].set_xlabel('Value')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(X.columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create boxplots for all features\n",
    "create_boxplots(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature_engineering\"></a>\n",
    "## 4. Feature Engineering\n",
    "\n",
    "In this section, we'll perform feature engineering to prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Select Features Based on High Correlation with the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select features based on correlation with the target\n",
    "def select_features(df, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Select features based on correlation with the target.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): Correlation threshold\n",
    "        \n",
    "    Returns:\n",
    "        list: List of selected feature names\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Warning: Target variable 'label' not found in the DataFrame\")\n",
    "        return df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation with the target\n",
    "    corr_with_target = df.corr()['label'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select features with correlation above the threshold\n",
    "    selected_features = corr_with_target[corr_with_target >= threshold].index.tolist()\n",
    "    \n",
    "    # Remove the target variable from the list of selected features\n",
    "    if 'label' in selected_features:\n",
    "        selected_features.remove('label')\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features with correlation >= {threshold}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Select features based on correlation with the target\n",
    "selected_features = select_features(df_scaled, threshold=0.1)\n",
    "\n",
    "# Display selected features and their correlation with the target\n",
    "if 'label' in df_scaled.columns:\n",
    "    corr_with_target = df_scaled.corr()['label'].abs().sort_values(ascending=False)\n",
    "    print(\"\\nSelected Features and Their Correlation with the Target:\")\n",
    "    print(corr_with_target[selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split Dataset into Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split dataset into features and target\n",
    "def split_features_target(df, selected_features=None):\n",
    "    \"\"\"\n",
    "    Split dataset into features (X) and target (y).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        selected_features (list): List of selected feature names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) where X is the feature matrix and y is the target vector\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Warning: Target variable 'label' not found in the DataFrame\")\n",
    "        return df, None\n",
    "    \n",
    "    # Use selected features if provided, otherwise use all features except the target\n",
    "    if selected_features is not None:\n",
    "        X = df[selected_features]\n",
    "    else:\n",
    "        X = df.drop(columns=['label'])\n",
    "    \n",
    "    # Extract the target variable\n",
    "    y = df['label']\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Split dataset into features and target\n",
    "X, y = split_features_target(df_scaled, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ml_phase\"></a>\n",
    "## 5. Machine Learning Phase\n",
    "\n",
    "In this section, we'll apply various machine learning algorithms to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, stratify=True):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target vector\n",
    "        test_size (float): Proportion of the dataset to include in the test split\n",
    "        stratify (bool): Whether to use stratified sampling\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if y is None:\n",
    "        print(\"Warning: Target variable is None, cannot split data\")\n",
    "        return X, None, None, None\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    if stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n",
    "        )\n",
    "        print(f\"Data split using stratified sampling (test_size={test_size})\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=RANDOM_STATE\n",
    "        )\n",
    "        print(f\"Data split using random sampling (test_size={test_size})\")\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, stratify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform K-Means clustering\n",
    "def perform_kmeans_clustering(X, max_k=10):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering and determine the optimal number of clusters.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        max_k (int): Maximum number of clusters to consider\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (kmeans, optimal_k)\n",
    "    \"\"\"\n",
    "    # Initialize lists to store metrics\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    # Compute metrics for different values of k\n",
    "    for k in range(2, max_k + 1):\n",
    "        # Fit K-Means\n",
    "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Compute inertia\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Compute silhouette score\n",
    "        silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "    \n",
    "    # Plot elbow method\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(2, max_k + 1), inertia, 'o-')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(2, max_k + 1), silhouette_scores, 'o-')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine optimal k\n",
    "    optimal_k_silhouette = np.argmax(silhouette_scores) + 2\n",
    "    print(f\"Optimal number of clusters (silhouette method): {optimal_k_silhouette}\")\n",
    "    \n",
    "    # Fit K-Means with optimal k\n",
    "    kmeans = KMeans(n_clusters=optimal_k_silhouette, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    return kmeans, optimal_k_silhouette\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans, optimal_k = perform_kmeans_clustering(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform dimensionality reduction with PCA\n",
    "def perform_pca(X, n_components=3):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction with PCA.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        n_components (int): Number of components\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (pca, X_pca)\n",
    "    \"\"\"\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Print explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"Explained variance ratio: {explained_variance}\")\n",
    "    print(f\"Total explained variance: {np.sum(explained_variance):.4f}\")\n",
    "    \n",
    "    return pca, X_pca\n",
    "\n",
    "# Perform dimensionality reduction with PCA\n",
    "pca, X_train_pca = perform_pca(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters\n",
    "def visualize_clusters(X_pca, labels, title='Clusters'):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA.\n",
    "    \n",
    "    Args:\n",
    "        X_pca (np.ndarray): PCA-transformed feature matrix\n",
    "        labels (np.ndarray): Cluster labels\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # 2D PCA scatter plot\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_xlabel('Principal Component 1')\n",
    "    ax1.set_ylabel('Principal Component 2')\n",
    "    ax1.set_title(f'2D PCA Scatter Plot - {title}')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Cluster')\n",
    "    \n",
    "    # 3D PCA scatter plot\n",
    "    if X_pca.shape[1] >= 3:\n",
    "        ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='viridis', alpha=0.8)\n",
    "        ax2.set_xlabel('Principal Component 1')\n",
    "        ax2.set_ylabel('Principal Component 2')\n",
    "        ax2.set_zlabel('Principal Component 3')\n",
    "        ax2.set_title(f'3D PCA Scatter Plot - {title}')\n",
    "        plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters\n",
    "visualize_clusters(X_train_pca, kmeans.labels_, title='K-Means Clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform linear regression\n",
    "def perform_linear_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Perform linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature matrix\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_train (pd.Series): Training target vector\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        \n",
    "    Returns:\n",
    "        LinearRegression: Fitted linear regression model\n",
    "    \"\"\"\n",
    "    # Check if the target variable is continuous\n",
    "    if y_train is None or y_test is None:\n",
    "        print(\"Warning: Target variable is None, cannot perform regression\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # Fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Linear Regression Results:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R Score: {r2:.4f}\")\n",
    "    \n",
    "    # Visualize regression with scatter plot and regression line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot of actual vs. predicted values\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Linear Regression: Actual vs. Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Perform linear regression\n",
    "lr_model = perform_linear_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate classification models\n",
    "def train_evaluate_classification_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classification models.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature matrix\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_train (pd.Series): Training target vector\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of fitted models\n",
    "        pd.DataFrame: DataFrame with model evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if y_train is None or y_test is None:\n",
    "        print(\"Warning: Target variable is None, cannot train classification models\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize dictionary to store models\n",
    "    models = {}\n",
    "    \n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    metrics = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': []\n",
    "    }\n",
    "    \n",
    "    # Define classification models\n",
    "    classification_models = {\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'Support Vector Machine': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE),\n",
    "        'CatBoost': cb.CatBoostClassifier(random_state=RANDOM_STATE, verbose=0),\n",
    "        'LightGBM': lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in classification_models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Store the model\n",
    "        models[name] = model\n",
    "        \n",
    "        # Store evaluation metrics\n",
    "        metrics['Model'].append(name)\n",
    "        metrics['Accuracy'].append(accuracy)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1 Score'].append(f1)\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Create a DataFrame with evaluation metrics\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df = metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return models, metrics_df\n",
    "\n",
    "# Train and evaluate classification models\n",
    "classification_models, metrics_df = train_evaluate_classification_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a recurrent neural network (RNN)\n",
    "def train_rnn(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a recurrent neural network (RNN) for classification.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature matrix\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_train (pd.Series): Training target vector\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        \n",
    "    Returns:\n",
    "        Sequential: Fitted RNN model\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if y_train is None or y_test is None:\n",
    "        print(\"Warning: Target variable is None, cannot train RNN\")\n",
    "        return None\n",
    "    \n",
    "    # Reshape input data for RNN (samples, time steps, features)\n",
    "    X_train_rnn = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_rnn = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # Determine the number of output units\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    output_units = 1 if n_classes == 2 else n_classes\n",
    "    activation = 'sigmoid' if n_classes == 2 else 'softmax'\n",
    "    loss = 'binary_crossentropy' if n_classes == 2 else 'sparse_categorical_crossentropy'\n",
    "    \n",
    "    # Build the RNN model\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(1, X_train.shape[1]), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(output_units, activation=activation)\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training RNN...\")\n",
    "    history = model.fit(\n",
    "        X_train_rnn, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_rnn, y_test, verbose=0)\n",
    "    print(f\"RNN Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train a recurrent neural network (RNN)\n",
    "rnn_model = train_rnn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 6. Model Evaluation\n",
    "\n",
    "In this section, we'll evaluate the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model evaluation metrics\n",
    "if metrics_df is not None:\n",
    "    print(\"Model Evaluation Metrics:\")\n",
    "    metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for a classification model.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted classification model\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "if classification_models is not None and metrics_df is not None:\n",
    "    best_model_name = metrics_df.iloc[0]['Model']\n",
    "    best_model = classification_models[best_model_name]\n",
    "    plot_confusion_matrix(best_model, X_test, y_test, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for a classification model.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted classification model\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    # Check if the model has predict_proba method\n",
    "    if not hasattr(model, 'predict_proba'):\n",
    "        print(f\"Warning: {model_name} does not have predict_proba method, cannot plot ROC curve\")\n",
    "        return\n",
    "    \n",
    "    # Get the number of classes\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    \n",
    "    # Binary classification\n",
    "    if n_classes == 2:\n",
    "        # Make predictions\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Compute ROC curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    # Multi-class classification\n",
    "    else:\n",
    "        # Make predictions\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        \n",
    "        # Compute ROC curve and AUC for each class\n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        \n",
    "        # One-vs-Rest approach\n",
    "        for i in range(n_classes):\n",
    "            # Convert to one-vs-rest binary classification problem\n",
    "            y_test_binary = (y_test == i).astype(int)\n",
    "            \n",
    "            # Compute ROC curve and AUC\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_binary, y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot ROC curve for each class\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.4f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Multi-class ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Plot ROC curve for the best model\n",
    "if classification_models is not None and metrics_df is not None:\n",
    "    best_model_name = metrics_df.iloc[0]['Model']\n",
    "    best_model = classification_models[best_model_name]\n",
    "    plot_roc_curve(best_model, X_test, y_test, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize model comparison\n",
    "def visualize_model_comparison(metrics_df):\n",
    "    \"\"\"\n",
    "    Visualize model comparison using bar plots.\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pd.DataFrame): DataFrame with model evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check if metrics_df exists\n",
    "    if metrics_df is None:\n",
    "        print(\"Warning: metrics_df is None, cannot visualize model comparison\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each metric\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Sort DataFrame by the current metric\n",
    "        df_sorted = metrics_df.sort_values(metric, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        sns.barplot(x='Model', y=metric, data=df_sorted, ax=axes[i])\n",
    "        \n",
    "        # Set title and labels\n",
    "        axes[i].set_title(f'Model Comparison - {metric}')\n",
    "        axes[i].set_xlabel('Model')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, v in enumerate(df_sorted[metric]):\n",
    "            axes[i].text(j, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize model comparison\n",
    "if metrics_df is not None:\n",
    "    visualize_model_comparison(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize decision tree\n",
    "def visualize_decision_tree(model, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Visualize the structure of a decision tree.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted decision tree model\n",
    "        feature_names (list): List of feature names\n",
    "        class_names (list): List of class names\n",
    "    \"\"\"\n",
    "    # Check if the model is a decision tree\n",
    "    if not isinstance(model, DecisionTreeClassifier):\n",
    "        print(\"Warning: Model is not a decision tree, cannot visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot the decision tree\n",
    "    plot_tree(model, feature_names=feature_names, class_names=class_names, filled=True, rounded=True)\n",
    "    \n",
    "    plt.title('Decision Tree Structure')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize decision tree\n",
    "if classification_models is not None and 'Decision Tree' in classification_models:\n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Get class names\n",
    "    if 'label' in label_encoders:\n",
    "        class_names = label_encoders['label'].classes_.tolist()\n",
    "    else:\n",
    "        class_names = [str(i) for i in range(len(np.unique(y)))]\n",
    "    \n",
    "    # Visualize decision tree\n",
    "    visualize_decision_tree(classification_models['Decision Tree'], feature_names, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"usage\"></a>\n",
    "## 7. Model Usage\n",
    "\n",
    "In this section, we'll demonstrate how to use the trained models for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict using all models\n",
    "def predict_with_all_models(sample, models, scaler, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Predict the class of a single data sample using all trained models.\n",
    "    \n",
    "    Args:\n",
    "        sample (pd.DataFrame): Single data sample\n",
    "        models (dict): Dictionary of fitted models\n",
    "        scaler (StandardScaler): Fitted scaler object\n",
    "        label_encoder (LabelEncoder): Fitted label encoder for the target variable\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with predictions from all models\n",
    "    \"\"\"\n",
    "    # Check if models exist\n",
    "    if models is None:\n",
    "        print(\"Warning: models is None, cannot make predictions\")\n",
    "        return None\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample)\n",
    "    \n",
    "    # Initialize dictionary to store predictions\n",
    "    predictions = {\n",
    "        'Model': [],\n",
    "        'Prediction': [],\n",
    "        'Confidence': []\n",
    "    }\n",
    "    \n",
    "    # Make predictions with each model\n",
    "    for name, model in models.items():\n",
    "        # Make prediction\n",
    "        pred = model.predict(sample_scaled)[0]\n",
    "        \n",
    "        # Get prediction confidence\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            prob = model.predict_proba(sample_scaled)[0]\n",
    "            confidence = prob[pred] if len(prob) > 1 else prob[0]\n",
    "        else:\n",
    "            confidence = None\n",
    "        \n",
    "        # Convert numeric prediction to class name\n",
    "        if label_encoder is not None:\n",
    "            pred_class = label_encoder.inverse_transform([pred])[0]\n",
    "        else:\n",
    "            pred_class = pred\n",
    "        \n",
    "        # Store prediction\n",
    "        predictions['Model'].append(name)\n",
    "        predictions['Prediction'].append(pred_class)\n",
    "        predictions['Confidence'].append(confidence)\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Demonstrate model usage with a sample from the test set\n",
    "if classification_models is not None and X_test is not None and y_test is not None:\n",
    "    # Get a sample from the test set\n",
    "    sample_idx = 0\n",
    "    sample = X_test.iloc[[sample_idx]]\n",
    "    true_label = y_test.iloc[sample_idx]\n",
    "    \n",
    "    # Convert numeric label to class name\n",
    "    if 'label' in label_encoders:\n",
    "        true_class = label_encoders['label'].inverse_transform([true_label])[0]\n",
    "    else:\n",
    "        true_class = true_label\n",
    "    \n",
    "    print(f\"True label: {true_class}\")\n",
    "    \n",
    "    # Predict with all models\n",
    "    predictions_df = predict_with_all_models(\n",
    "        sample, classification_models, scaler, \n",
    "        label_encoders['label'] if 'label' in label_encoders else None\n",
    "    )\n",
    "    \n",
    "    # Display predictions\n",
    "    if predictions_df is not None:\n",
    "        print(\"\\nPredictions from all models:\")\n",
    "        predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement a prediction pipeline\n",
    "def prediction_pipeline(data, selected_features, scaler, model, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Implement a prediction pipeline for a single data sample.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data\n",
    "        selected_features (list): List of selected feature names\n",
    "        scaler (StandardScaler): Fitted scaler object\n",
    "        model: Fitted model\n",
    "        label_encoder (LabelEncoder): Fitted label encoder for the target variable\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (prediction, confidence)\n",
    "    \"\"\"\n",
    "    # Select features\n",
    "    X = data[selected_features]\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_scaled)[0]\n",
    "    \n",
    "    # Get prediction confidence\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        prob = model.predict_proba(X_scaled)[0]\n",
    "        confidence = prob[pred] if len(prob) > 1 else prob[0]\n",
    "    else:\n",
    "        confidence = None\n",
    "    \n",
    "    # Convert numeric prediction to class name\n",
    "    if label_encoder is not None:\n",
    "        pred_class = label_encoder.inverse_transform([pred])[0]\n",
    "    else:\n",
    "        pred_class = pred\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Example usage of the prediction pipeline\n",
    "print(\"Example Usage of the Prediction Pipeline:\")\n",
    "print(\"\"\"# Load and preprocess new data\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "new_data = preprocess_data(new_data)  # Apply the same preprocessing steps\n",
    "\n",
    "# Make prediction\n",
    "prediction, confidence = prediction_pipeline(\n",
    "    new_data, selected_features, scaler, best_model, \n",
    "    label_encoders['label'] if 'label' in label_encoders else None\n",
    ")\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}