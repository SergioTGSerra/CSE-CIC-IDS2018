{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline for Network Intrusion Detection\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for network intrusion detection using the CSE-CIC-IDS2018 dataset. The pipeline includes data preprocessing, exploratory data analysis, feature engineering, and various machine learning models for classification.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Preprocessing and Transformation](#preprocessing)\n",
    "3. [Exploratory Data Analysis (EDA)](#eda)\n",
    "4. [Feature Engineering](#feature_engineering)\n",
    "5. [Machine Learning Phase](#ml_phase)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Model Usage](#usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup and Imports\n",
    "\n",
    "First, we'll import all the necessary libraries for our machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve, auc, \n",
    "    silhouette_score, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Advanced ML models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## 2. Data Preprocessing and Transformation\n",
    "\n",
    "In this section, we'll load the dataset, inspect it, clean it, and prepare it for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Sample Dataset\n",
    "\n",
    "Since we're working with a limited environment, we'll create a sample dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Create a sample dataset for demonstration purposes.\n",
    "    \n",
    "    Args:\n",
    "        n_samples (int): Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Sample DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create sample features\n",
    "    sample_data = {\n",
    "        'Flow Duration': np.random.randint(1, 100000, n_samples),\n",
    "        'Total Fwd Packets': np.random.randint(1, 100, n_samples),\n",
    "        'Total Backward Packets': np.random.randint(1, 100, n_samples),\n",
    "        'Total Length of Fwd Packets': np.random.randint(1, 10000, n_samples),\n",
    "        'Total Length of Bwd Packets': np.random.randint(1, 10000, n_samples),\n",
    "        'Fwd Packet Length Max': np.random.randint(1, 1500, n_samples),\n",
    "        'Fwd Packet Length Min': np.random.randint(0, 100, n_samples),\n",
    "        'Fwd Packet Length Mean': np.random.uniform(10, 500, n_samples),\n",
    "        'Bwd Packet Length Max': np.random.randint(1, 1500, n_samples),\n",
    "        'Bwd Packet Length Min': np.random.randint(0, 100, n_samples),\n",
    "        'Bwd Packet Length Mean': np.random.uniform(10, 500, n_samples),\n",
    "        'Flow Bytes/s': np.random.uniform(0, 10000, n_samples),\n",
    "        'Flow Packets/s': np.random.uniform(0, 1000, n_samples),\n",
    "        'Flow IAT Mean': np.random.uniform(0, 1000, n_samples),\n",
    "        'Flow IAT Std': np.random.uniform(0, 500, n_samples),\n",
    "        'Flow IAT Max': np.random.uniform(0, 2000, n_samples),\n",
    "        'Flow IAT Min': np.random.uniform(0, 100, n_samples),\n",
    "        'Fwd Header Length.1': np.random.randint(20, 100, n_samples),  # Duplicate column\n",
    "        'Fwd Header Length': np.random.randint(20, 100, n_samples),\n",
    "        'Protocol': np.random.choice(['TCP', 'UDP', 'ICMP'], n_samples),\n",
    "        'Destination Port': np.random.choice([80, 443, 22, 53, 8080], n_samples),\n",
    "        'Label': np.random.choice(['BENIGN', 'DoS Hulk', 'PortScan', 'Brute Force-Web', 'Web Attack'], n_samples, \n",
    "                                 p=[0.7, 0.1, 0.1, 0.05, 0.05])\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create a sample dataset for demonstration\n",
    "df = create_sample_dataset(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Inspect Data Types and Count Non-Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "df.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()\n",
    "\n",
    "# Count non-null values for each column\n",
    "print(\"\\nNon-null value counts:\")\n",
    "non_null_counts = df.count()\n",
    "print(non_null_counts)\n",
    "\n",
    "# Calculate percentage of non-null values\n",
    "non_null_percentage = (non_null_counts / len(df)) * 100\n",
    "print(\"\\nPercentage of non-null values:\")\n",
    "print(non_null_percentage)\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize column names by removing leading/trailing whitespace,\n",
    "    replacing spaces with underscores, and converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized column names\n",
    "    \"\"\"\n",
    "    # Store original column names for reference\n",
    "    original_columns = df.columns.tolist()\n",
    "    \n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    \n",
    "    # Print mapping of original to normalized column names\n",
    "    print(\"Column name mapping:\")\n",
    "    for orig, norm in zip(original_columns, df.columns):\n",
    "        if orig != norm:\n",
    "            print(f\"  {orig} -> {norm}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Normalize column names\n",
    "df = normalize_column_names(df)\n",
    "\n",
    "# Display the first few rows with normalized column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Identify and Remove Duplicate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify and remove duplicate columns\n",
    "def remove_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Identify and remove duplicate columns from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicate columns removed\n",
    "    \"\"\"\n",
    "    # Get list of all columns\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Initialize list to store duplicate columns\n",
    "    duplicate_columns = []\n",
    "    \n",
    "    # Check for columns ending with '.1', '.2', etc.\n",
    "    for col in columns:\n",
    "        if col.endswith(('.1', '.2', '.3', '.4', '.5')):\n",
    "            base_col = col.rsplit('.', 1)[0]\n",
    "            if base_col in columns:\n",
    "                # Check if the columns are actually duplicates\n",
    "                if df[col].equals(df[base_col]):\n",
    "                    duplicate_columns.append(col)\n",
    "                    print(f\"Found duplicate column: {col} (duplicate of {base_col})\")\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    if duplicate_columns:\n",
    "        df = df.drop(columns=duplicate_columns)\n",
    "        print(f\"Removed {len(duplicate_columns)} duplicate columns\")\n",
    "    else:\n",
    "        print(\"No duplicate columns found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Remove duplicate columns\n",
    "df = remove_duplicate_columns(df)\n",
    "\n",
    "# Display the shape of the DataFrame after removing duplicate columns\n",
    "print(f\"DataFrame shape after removing duplicate columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Convert Categorical Features using Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical features\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Encode categorical features using Label Encoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded categorical features\n",
    "        dict: Dictionary mapping column names to their respective label encoders\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Initialize dictionary to store label encoders\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Identify categorical columns (excluding the target variable)\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if 'label' in categorical_columns:\n",
    "        categorical_columns.remove('label')\n",
    "    \n",
    "    print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
    "    \n",
    "    # Encode each categorical column\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    # Handle the target variable separately\n",
    "    if 'label' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        label_encoders['label'] = le\n",
    "        print(f\"Encoded label: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded, label_encoders = encode_categorical_features(df)\n",
    "\n",
    "# Display the first few rows of the encoded DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Convert All Data Types to Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert all data types to float\n",
    "def convert_to_float(df):\n",
    "    \"\"\"\n",
    "    Convert all columns (except the target variable) to float.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all columns converted to float\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_float = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "    \n",
    "    # Convert each column to float\n",
    "    for col in columns:\n",
    "        df_float[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
    "    \n",
    "    return df_float\n",
    "\n",
    "# Convert all data types to float\n",
    "df_float = convert_to_float(df_encoded)\n",
    "\n",
    "# Display the data types after conversion\n",
    "print(\"Data types after conversion:\")\n",
    "print(df_float.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Replace Infinite Values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace infinite values with NaN\n",
    "def replace_inf_with_nan(df):\n",
    "    \"\"\"\n",
    "    Replace infinite values with NaN in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with infinite values replaced with NaN\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_inf = df.copy()\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    df_no_inf = df_no_inf.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Count NaN values after replacement\n",
    "    nan_counts = df_no_inf.isna().sum()\n",
    "    print(\"NaN counts after replacing infinite values:\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "    \n",
    "    return df_no_inf\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_no_inf = replace_inf_with_nan(df_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Handle Missing Values (NaN) by Imputing with the Column Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values with column mean\n",
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values (NaN) with the column mean.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values imputed\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "    \n",
    "    # Impute missing values with column mean\n",
    "    for col in columns:\n",
    "        if df_imputed[col].isna().any():\n",
    "            mean_value = df_imputed[col].mean()\n",
    "            df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
    "            print(f\"Imputed {df_imputed[col].isna().sum()} missing values in {col} with mean: {mean_value:.4f}\")\n",
    "    \n",
    "    # Verify that there are no more missing values\n",
    "    nan_counts = df_imputed.isna().sum()\n",
    "    if nan_counts.sum() > 0:\n",
    "        print(\"Warning: There are still missing values in the DataFrame\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    else:\n",
    "        print(\"All missing values have been imputed\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Impute missing values with column mean\n",
    "df_imputed = impute_missing_values(df_no_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Replace Nonsensical Negative Values with the Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace nonsensical negative values with the median\n",
    "def replace_negative_values(df):\n",
    "    \"\"\"\n",
    "    Replace nonsensical negative values in specific columns with the median.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with nonsensical negative values replaced\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_neg = df.copy()\n",
    "    \n",
    "    # List of columns that should not have negative values\n",
    "    # These are typically columns representing counts, durations, lengths, etc.\n",
    "    non_negative_columns = [\n",
    "        col for col in df.columns if any(keyword in col.lower() for keyword in \n",
    "                                        ['duration', 'length', 'packets', 'bytes', 'count', 'min', 'max', 'mean'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(non_negative_columns)} columns that should not have negative values\")\n",
    "    \n",
    "    # Replace negative values with the median in each column\n",
    "    for col in non_negative_columns:\n",
    "        if col in df_no_neg.columns and (df_no_neg[col] < 0).any():\n",
    "            neg_count = (df_no_neg[col] < 0).sum()\n",
    "            median_value = df_no_neg[df_no_neg[col] >= 0][col].median()\n",
    "            df_no_neg.loc[df_no_neg[col] < 0, col] = median_value\n",
    "            print(f\"Replaced {neg_count} negative values in {col} with median: {median_value:.4f}\")\n",
    "    \n",
    "    return df_no_neg\n",
    "\n",
    "# Replace nonsensical negative values with the median\n",
    "df_no_neg = replace_negative_values(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Detect and Remove Outliers using Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and remove outliers using Isolation Forest\n",
    "def remove_outliers(df, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect and remove outliers using the Isolation Forest algorithm.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        contamination (float): The proportion of outliers in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_no_outliers = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Initialize and fit the Isolation Forest model\n",
    "    isolation_forest = IsolationForest(contamination=contamination, random_state=RANDOM_STATE)\n",
    "    outlier_pred = isolation_forest.fit_predict(X)\n",
    "    \n",
    "    # Count outliers\n",
    "    outlier_count = (outlier_pred == -1).sum()\n",
    "    print(f\"Detected {outlier_count} outliers ({outlier_count/len(df)*100:.2f}% of the dataset)\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_no_outliers = df_no_outliers[outlier_pred == 1]\n",
    "    print(f\"DataFrame shape after removing outliers: {df_no_outliers.shape}\")\n",
    "    \n",
    "    return df_no_outliers\n",
    "\n",
    "# Detect and remove outliers\n",
    "df_no_outliers = remove_outliers(df_no_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Scale All Numerical Features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale numerical features\n",
    "def scale_features(df):\n",
    "    \"\"\"\n",
    "    Scale all numerical features using StandardScaler.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with scaled features\n",
    "        StandardScaler: Fitted scaler object\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Initialize and fit the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create a new DataFrame with scaled features\n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    # Add the target variable back to the DataFrame\n",
    "    if 'label' in df.columns:\n",
    "        df_scaled['label'] = df['label'].values\n",
    "    \n",
    "    print(f\"Features scaled using StandardScaler\")\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Scale numerical features\n",
    "df_scaled, scaler = scale_features(df_no_outliers)\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Summary of Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of data preprocessing steps\n",
    "print(\"Summary of Data Preprocessing Steps:\")\n",
    "print(f\"1. Original DataFrame shape: {df.shape}\")\n",
    "print(f\"2. After removing duplicate columns: {df_encoded.shape}\")\n",
    "print(f\"3. After handling missing values: {df_imputed.shape}\")\n",
    "print(f\"4. After removing outliers: {df_no_outliers.shape}\")\n",
    "print(f\"5. Final DataFrame shape: {df_scaled.shape}\")\n",
    "\n",
    "# Display class distribution\n",
    "if 'label' in df_scaled.columns:\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    class_counts = df_scaled['label'].value_counts()\n",
    "    class_percentages = class_counts / len(df_scaled) * 100\n",
    "    \n",
    "    # Create a DataFrame to display class distribution\n",
    "    class_distribution = pd.DataFrame({\n",
    "        'Count': class_counts,\n",
    "        'Percentage': class_percentages\n",
    "    })\n",
    "    \n",
    "    # Map numeric labels back to original class names\n",
    "    if 'label' in label_encoders:\n",
    "        class_distribution.index = [label_encoders['label'].inverse_transform([i])[0] for i in class_distribution.index]\n",
    "    \n",
    "    print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we'll perform exploratory data analysis to understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute descriptive statistics\n",
    "def compute_descriptive_statistics(df):\n",
    "    \"\"\"\n",
    "    Compute descriptive statistics for the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with descriptive statistics\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Compute descriptive statistics\n",
    "    stats = X.describe(percentiles=[0.25, 0.5, 0.75])\n",
    "    \n",
    "    # Add mode to the statistics\n",
    "    mode = X.mode().iloc[0]\n",
    "    stats.loc['mode'] = mode\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compute descriptive statistics\n",
    "stats = compute_descriptive_statistics(df_no_outliers)\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize descriptive statistics\n",
    "def visualize_descriptive_statistics(stats):\n",
    "    \"\"\"\n",
    "    Visualize descriptive statistics using bar plots.\n",
    "    \n",
    "    Args:\n",
    "        stats (pd.DataFrame): DataFrame with descriptive statistics\n",
    "    \"\"\"\n",
    "    # Select a subset of columns for visualization\n",
    "    selected_columns = stats.columns[:5]  # Select first 5 columns\n",
    "    \n",
    "    # Select statistics to visualize\n",
    "    selected_stats = ['min', '25%', '50%', '75%', 'max', 'mean', 'std']\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(len(selected_columns), 1, figsize=(12, 4 * len(selected_columns)))\n",
    "    \n",
    "    # Plot each column's statistics\n",
    "    for i, col in enumerate(selected_columns):\n",
    "        ax = axes[i] if len(selected_columns) > 1 else axes\n",
    "        \n",
    "        # Extract statistics for the column\n",
    "        col_stats = stats.loc[selected_stats, col]\n",
    "        \n",
    "        # Create bar plot\n",
    "        col_stats.plot(kind='bar', ax=ax)\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Descriptive Statistics for {col}')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_xlabel('Statistic')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, v in enumerate(col_stats):\n",
    "            ax.text(j, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize descriptive statistics\n",
    "visualize_descriptive_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot histograms for selected features\n",
    "def plot_histograms(df, n_cols=5):\n",
    "    \"\"\"\n",
    "    Plot histograms for selected features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        n_cols (int): Number of columns to visualize\n",
    "    \"\"\"\n",
    "    # Get list of all columns except the target variable\n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    # Select a subset of columns for visualization\n",
    "    selected_columns = X.columns[:n_cols]  # Select first n_cols columns\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))\n",
    "    \n",
    "    # Plot histogram for each column\n",
    "    for i, col in enumerate(selected_columns):\n",
    "        ax = axes[i] if n_cols > 1 else axes\n",
    "        \n",
    "        # Plot histogram\n",
    "        sns.histplot(df[col], kde=True, ax=ax)\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Distribution of {col}')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot histograms for selected features\n",
    "plot_histograms(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a correlation matrix heatmap\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create a correlation matrix heatmap.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \"\"\"\n",
    "    # Get list of all columns including the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a correlation matrix heatmap\n",
    "create_correlation_matrix(df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a filtered heatmap for high correlations\n",
    "def create_filtered_heatmap(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Create a filtered heatmap for high correlations.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): Correlation threshold\n",
    "    \"\"\"\n",
    "    # Get list of all columns including the target variable\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Create a filtered correlation matrix\n",
    "    filtered_corr = corr_matrix.mask(np.abs(corr_matrix) < threshold)\n",
    "    filtered_corr = filtered_corr.mask(mask)\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5, fmt='.2f')\n",
    "    plt.title(f'Filtered Correlation Matrix Heatmap (|corr| >= {threshold})')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a filtered heatmap for high correlations (threshold = 0.5)\n",
    "create_filtered_heatmap(df_no_outliers, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature_engineering\"></a>\n",
    "## 4. Feature Engineering\n",
    "\n",
    "In this section, we'll perform feature engineering to prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Select Features Based on High Correlation with the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select features based on correlation with the target\n",
    "def select_features(df, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Select features based on correlation with the target.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): Correlation threshold\n",
    "        \n",
    "    Returns:\n",
    "        list: List of selected feature names\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Warning: Target variable 'label' not found in the DataFrame\")\n",
    "        return df.columns.tolist()\n",
    "    \n",
    "    # Compute correlation with the target\n",
    "    corr_with_target = df.corr()['label'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select features with correlation above the threshold\n",
    "    selected_features = corr_with_target[corr_with_target >= threshold].index.tolist()\n",
    "    \n",
    "    # Remove the target variable from the list of selected features\n",
    "    if 'label' in selected_features:\n",
    "        selected_features.remove('label')\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features with correlation >= {threshold}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Select features based on correlation with the target\n",
    "selected_features = select_features(df_scaled, threshold=0.1)\n",
    "\n",
    "# Display selected features and their correlation with the target\n",
    "if 'label' in df_scaled.columns:\n",
    "    corr_with_target = df_scaled.corr()['label'].abs().sort_values(ascending=False)\n",
    "    print(\"\\nSelected Features and Their Correlation with the Target:\")\n",
    "    print(corr_with_target[selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split Dataset into Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split dataset into features and target\n",
    "def split_features_target(df, selected_features=None):\n",
    "    \"\"\"\n",
    "    Split dataset into features (X) and target (y).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        selected_features (list): List of selected feature names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) where X is the feature matrix and y is the target vector\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Warning: Target variable 'label' not found in the DataFrame\")\n",
    "        return df, None\n",
    "    \n",
    "    # Use selected features if provided, otherwise use all features except the target\n",
    "    if selected_features is not None:\n",
    "        X = df[selected_features]\n",
    "    else:\n",
    "        X = df.drop(columns=['label'])\n",
    "    \n",
    "    # Extract the target variable\n",
    "    y = df['label']\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Split dataset into features and target\n",
    "X, y = split_features_target(df_scaled, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ml_phase\"></a>\n",
    "## 5. Machine Learning Phase\n",
    "\n",
    "In this section, we'll apply various machine learning algorithms to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, stratify=True):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target vector\n",
    "        test_size (float): Proportion of the dataset to include in the test split\n",
    "        stratify (bool): Whether to use stratified sampling\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if y is None:\n",
    "        print(\"Warning: Target variable is None, cannot split data\")\n",
    "        return X, None, None, None\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    if stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n",
    "        )\n",
    "        print(f\"Data split using stratified sampling (test_size={test_size})\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=RANDOM_STATE\n",
    "        )\n",
    "        print(f\"Data split using random sampling (test_size={test_size})\")\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, stratify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate classification models\n",
    "def train_evaluate_classification_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classification models.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature matrix\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_train (pd.Series): Training target vector\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of fitted models\n",
    "        pd.DataFrame: DataFrame with model evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check if the target variable exists\n",
    "    if y_train is None or y_test is None:\n",
    "        print(\"Warning: Target variable is None, cannot train classification models\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize dictionary to store models\n",
    "    models = {}\n",
    "    \n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    metrics = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': []\n",
    "    }\n",
    "    \n",
    "    # Define classification models\n",
    "    classification_models = {\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in classification_models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Store the model\n",
    "        models[name] = model\n",
    "        \n",
    "        # Store evaluation metrics\n",
    "        metrics['Model'].append(name)\n",
    "        metrics['Accuracy'].append(accuracy)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1 Score'].append(f1)\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Create a DataFrame with evaluation metrics\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df = metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return models, metrics_df\n",
    "\n",
    "# Train and evaluate classification models\n",
    "classification_models, metrics_df = train_evaluate_classification_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 6. Model Evaluation\n",
    "\n",
    "In this section, we'll evaluate the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model evaluation metrics\n",
    "if metrics_df is not None:\n",
    "    print(\"Model Evaluation Metrics:\")\n",
    "    metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for a classification model.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted classification model\n",
    "        X_test (pd.DataFrame): Testing feature matrix\n",
    "        y_test (pd.Series): Testing target vector\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "if classification_models is not None and metrics_df is not None:\n",
    "    best_model_name = metrics_df.iloc[0]['Model']\n",
    "    best_model = classification_models[best_model_name]\n",
    "    plot_confusion_matrix(best_model, X_test, y_test, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize model comparison\n",
    "def visualize_model_comparison(metrics_df):\n",
    "    \"\"\"\n",
    "    Visualize model comparison using bar plots.\n",
    "    \n",
    "    Args:\n",
    "        metrics_df (pd.DataFrame): DataFrame with model evaluation metrics\n",
    "    \"\"\"\n",
    "    # Check if metrics_df exists\n",
    "    if metrics_df is None:\n",
    "        print(\"Warning: metrics_df is None, cannot visualize model comparison\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each metric\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Sort DataFrame by the current metric\n",
    "        df_sorted = metrics_df.sort_values(metric, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        sns.barplot(x='Model', y=metric, data=df_sorted, ax=axes[i])\n",
    "        \n",
    "        # Set title and labels\n",
    "        axes[i].set_title(f'Model Comparison - {metric}')\n",
    "        axes[i].set_xlabel('Model')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for j, v in enumerate(df_sorted[metric]):\n",
    "            axes[i].text(j, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize model comparison\n",
    "if metrics_df is not None:\n",
    "    visualize_model_comparison(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"usage\"></a>\n",
    "## 7. Model Usage\n",
    "\n",
    "In this section, we'll demonstrate how to use the trained models for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict using all models\n",
    "def predict_with_all_models(sample, models, scaler, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Predict the class of a single data sample using all trained models.\n",
    "    \n",
    "    Args:\n",
    "        sample (pd.DataFrame): Single data sample\n",
    "        models (dict): Dictionary of fitted models\n",
    "        scaler (StandardScaler): Fitted scaler object\n",
    "        label_encoder (LabelEncoder): Fitted label encoder for the target variable\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with predictions from all models\n",
    "    \"\"\"\n",
    "    # Check if models exist\n",
    "    if models is None:\n",
    "        print(\"Warning: models is None, cannot make predictions\")\n",
    "        return None\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample)\n",
    "    \n",
    "    # Initialize dictionary to store predictions\n",
    "    predictions = {\n",
    "        'Model': [],\n",
    "        'Prediction': [],\n",
    "        'Confidence': []\n",
    "    }\n",
    "    \n",
    "    # Make predictions with each model\n",
    "    for name, model in models.items():\n",
    "        # Make prediction\n",
    "        pred = model.predict(sample_scaled)[0]\n",
    "        \n",
    "        # Get prediction confidence\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            prob = model.predict_proba(sample_scaled)[0]\n",
    "            confidence = prob[pred] if len(prob) > 1 else prob[0]\n",
    "        else:\n",
    "            confidence = None\n",
    "        \n",
    "        # Convert numeric prediction to class name\n",
    "        if label_encoder is not None:\n",
    "            pred_class = label_encoder.inverse_transform([pred])[0]\n",
    "        else:\n",
    "            pred_class = pred\n",
    "        \n",
    "        # Store prediction\n",
    "        predictions['Model'].append(name)\n",
    "        predictions['Prediction'].append(pred_class)\n",
    "        predictions['Confidence'].append(confidence)\n",
    "    \n",
    "    # Create a DataFrame with predictions\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Demonstrate model usage with a sample from the test set\n",
    "if classification_models is not None and X_test is not None and y_test is not None:\n",
    "    # Get a sample from the test set\n",
    "    sample_idx = 0\n",
    "    sample = X_test.iloc[[sample_idx]]\n",
    "    true_label = y_test.iloc[sample_idx]\n",
    "    \n",
    "    # Convert numeric label to class name\n",
    "    if 'label' in label_encoders:\n",
    "        true_class = label_encoders['label'].inverse_transform([true_label])[0]\n",
    "    else:\n",
    "        true_class = true_label\n",
    "    \n",
    "    print(f\"True label: {true_class}\")\n",
    "    \n",
    "    # Predict with all models\n",
    "    predictions_df = predict_with_all_models(\n",
    "        sample, classification_models, scaler, \n",
    "        label_encoders['label'] if 'label' in label_encoders else None\n",
    "    )\n",
    "    \n",
    "    # Display predictions\n",
    "    if predictions_df is not None:\n",
    "        print(\"\\nPredictions from all models:\")\n",
    "        predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement a prediction pipeline\n",
    "def prediction_pipeline(data, selected_features, scaler, model, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Implement a prediction pipeline for a single data sample.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data\n",
    "        selected_features (list): List of selected feature names\n",
    "        scaler (StandardScaler): Fitted scaler object\n",
    "        model: Fitted model\n",
    "        label_encoder (LabelEncoder): Fitted label encoder for the target variable\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (prediction, confidence)\n",
    "    \"\"\"\n",
    "    # Select features\n",
    "    X = data[selected_features]\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_scaled)[0]\n",
    "    \n",
    "    # Get prediction confidence\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        prob = model.predict_proba(X_scaled)[0]\n",
    "        confidence = prob[pred] if len(prob) > 1 else prob[0]\n",
    "    else:\n",
    "        confidence = None\n",
    "    \n",
    "    # Convert numeric prediction to class name\n",
    "    if label_encoder is not None:\n",
    "        pred_class = label_encoder.inverse_transform([pred])[0]\n",
    "    else:\n",
    "        pred_class = pred\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Example usage of the prediction pipeline\n",
    "print(\"Example Usage of the Prediction Pipeline:\")\n",
    "print(\"\"\"# Load and preprocess new data\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "new_data = preprocess_data(new_data)  # Apply the same preprocessing steps\n",
    "\n",
    "# Make prediction\n",
    "prediction, confidence = prediction_pipeline(\n",
    "    new_data, selected_features, scaler, best_model, \n",
    "    label_encoders['label'] if 'label' in label_encoders else None\n",
    ")\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}