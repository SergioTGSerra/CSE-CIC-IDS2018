{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline for Network Intrusion Detection - Test Version\n",
    "\n",
    "This is a test version of the notebook that loads only 200 records from the dataset to verify functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(sample_size=200):\n",
    "    \"\"\"\n",
    "    Create a sample dataset for testing purposes.\n",
    "    \n",
    "    Args:\n",
    "        sample_size (int): Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Sample DataFrame\n",
    "    \"\"\"\n",
    "    # Create sample features\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sample_data = {\n",
    "        'Flow Duration': np.random.randint(1, 100000, sample_size),\n",
    "        'Total Fwd Packets': np.random.randint(1, 100, sample_size),\n",
    "        'Total Backward Packets': np.random.randint(1, 100, sample_size),\n",
    "        'Total Length of Fwd Packets': np.random.randint(1, 10000, sample_size),\n",
    "        'Total Length of Bwd Packets': np.random.randint(1, 10000, sample_size),\n",
    "        'Fwd Packet Length Max': np.random.randint(1, 1500, sample_size),\n",
    "        'Fwd Packet Length Min': np.random.randint(0, 100, sample_size),\n",
    "        'Fwd Packet Length Mean': np.random.uniform(10, 500, sample_size),\n",
    "        'Bwd Packet Length Max': np.random.randint(1, 1500, sample_size),\n",
    "        'Bwd Packet Length Min': np.random.randint(0, 100, sample_size),\n",
    "        'Bwd Packet Length Mean': np.random.uniform(10, 500, sample_size),\n",
    "        'Flow Bytes/s': np.random.uniform(0, 10000, sample_size),\n",
    "        'Flow Packets/s': np.random.uniform(0, 1000, sample_size),\n",
    "        'Flow IAT Mean': np.random.uniform(0, 1000, sample_size),\n",
    "        'Flow IAT Std': np.random.uniform(0, 500, sample_size),\n",
    "        'Flow IAT Max': np.random.uniform(0, 2000, sample_size),\n",
    "        'Flow IAT Min': np.random.uniform(0, 100, sample_size),\n",
    "        'Fwd Header Length.1': np.random.randint(20, 100, sample_size),  # Duplicate column\n",
    "        'Fwd Header Length': np.random.randint(20, 100, sample_size),\n",
    "        'Protocol': np.random.choice(['TCP', 'UDP', 'ICMP'], sample_size),\n",
    "        'Destination Port': np.random.choice([80, 443, 22, 53, 8080], sample_size),\n",
    "        'Label': np.random.choice(['BENIGN', 'DoS', 'PortScan', 'Brute Force', 'Web Attack'], sample_size, \n",
    "                                 p=[0.7, 0.1, 0.1, 0.05, 0.05])\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Created sample dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load sample data\n",
    "df = load_sample_data(200)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize column names\n",
    "def normalize_column_names(df):\n",
    "    \"\"\"\n",
    "    Normalize column names by removing leading/trailing whitespace,\n",
    "    replacing spaces with underscores, and converting to lowercase.\n",
    "    \"\"\"\n",
    "    original_columns = df.columns.tolist()\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    \n",
    "    print(\"Column name mapping:\")\n",
    "    for orig, norm in zip(original_columns, df.columns):\n",
    "        if orig != norm:\n",
    "            print(f\"  {orig} -> {norm}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Normalize column names\n",
    "df = normalize_column_names(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns\n",
    "def remove_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Identify and remove duplicate columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    duplicate_columns = []\n",
    "    \n",
    "    for col in columns:\n",
    "        if col.endswith(('.1', '.2', '.3', '.4', '.5')):\n",
    "            base_col = col.rsplit('.', 1)[0]\n",
    "            if base_col in columns:\n",
    "                if df[col].equals(df[base_col]):\n",
    "                    duplicate_columns.append(col)\n",
    "                    print(f\"Found duplicate column: {col} (duplicate of {base_col})\")\n",
    "    \n",
    "    if duplicate_columns:\n",
    "        df = df.drop(columns=duplicate_columns)\n",
    "        print(f\"Removed {len(duplicate_columns)} duplicate columns\")\n",
    "    else:\n",
    "        print(\"No duplicate columns found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Remove duplicate columns\n",
    "df = remove_duplicate_columns(df)\n",
    "print(f\"DataFrame shape after removing duplicate columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Encode categorical features using Label Encoding.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if 'label' in categorical_columns:\n",
    "        categorical_columns.remove('label')\n",
    "    \n",
    "    print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        label_encoders['label'] = le\n",
    "        print(f\"Encoded label: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded, label_encoders = encode_categorical_features(df)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all data types to float\n",
    "def convert_to_float(df):\n",
    "    \"\"\"\n",
    "    Convert all columns (except the target variable) to float.\n",
    "    \"\"\"\n",
    "    df_float = df.copy()\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "    \n",
    "    for col in columns:\n",
    "        df_float[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
    "    \n",
    "    return df_float\n",
    "\n",
    "# Convert all data types to float\n",
    "df_float = convert_to_float(df_encoded)\n",
    "print(\"Data types after conversion:\")\n",
    "print(df_float.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "def scale_features(df):\n",
    "    \"\"\"\n",
    "    Scale all numerical features using StandardScaler.\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    X = df.drop(columns=['label'] if 'label' in df.columns else [])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        df_scaled['label'] = df['label'].values\n",
    "    \n",
    "    print(f\"Features scaled using StandardScaler\")\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Scale numerical features\n",
    "df_scaled, scaler = scale_features(df_float)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution\n",
    "if 'label' in df_scaled.columns:\n",
    "    print(\"Class Distribution:\")\n",
    "    class_counts = df_scaled['label'].value_counts()\n",
    "    class_percentages = class_counts / len(df_scaled) * 100\n",
    "    \n",
    "    class_distribution = pd.DataFrame({\n",
    "        'Count': class_counts,\n",
    "        'Percentage': class_percentages\n",
    "    })\n",
    "    \n",
    "    if 'label' in label_encoders:\n",
    "        class_distribution.index = [label_encoders['label'].inverse_transform([i])[0] for i in class_distribution.index]\n",
    "    \n",
    "    print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df_scaled.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features and target\n",
    "X = df_scaled.drop(columns=['label'])\n",
    "y = df_scaled['label']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict using the trained model\n",
    "def predict_sample(sample, model, scaler, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Predict the class of a single data sample.\n",
    "    \"\"\"\n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(sample_scaled)[0]\n",
    "    \n",
    "    # Get prediction confidence\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        prob = model.predict_proba(sample_scaled)[0]\n",
    "        confidence = prob[pred]\n",
    "    else:\n",
    "        confidence = None\n",
    "    \n",
    "    # Convert numeric prediction to class name\n",
    "    if label_encoder is not None:\n",
    "        pred_class = label_encoder.inverse_transform([pred])[0]\n",
    "    else:\n",
    "        pred_class = pred\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Get a sample from the test set\n",
    "sample_idx = 0\n",
    "sample = X_test.iloc[[sample_idx]]\n",
    "true_label = y_test.iloc[sample_idx]\n",
    "\n",
    "# Convert numeric label to class name\n",
    "if 'label' in label_encoders:\n",
    "    true_class = label_encoders['label'].inverse_transform([true_label])[0]\n",
    "else:\n",
    "    true_class = true_label\n",
    "\n",
    "# Predict with the model\n",
    "pred_class, confidence = predict_sample(\n",
    "    sample, rf, scaler, \n",
    "    label_encoders['label'] if 'label' in label_encoders else None\n",
    ")\n",
    "\n",
    "print(f\"True label: {true_class}\")\n",
    "print(f\"Predicted label: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}